# -*- coding: utf-8 -*-
"""House price prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r7EM-tK8gGnUoWxZIvIWviY1vtg2r1E9

# In this notebook, I am going to perform EDA, Feature Engineering and Model Prediction for the Housing prices dataset
"""

!pip install kaggle

from google.colab import files
files.upload()  # This will prompt you to upload the kaggle.json file

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d rajarshiroy0123/house-prices-in-india-2023

!unzip house-prices-in-india-2023.zip

import pandas as pd
df = pd.read_csv('Scraped_Data.csv')

# Display the first few rows of the dataset
print(df.shape)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import folium
from folium import  Marker
from folium.plugins import  MarkerCluster
import math

"""# In this Notebook, I would have 3 sub-tasks
1. Spliting the Main_dataset into 2 datasets 1 containing null values in the target value("exactPrice"), and other no containing null values
   
2. Copy the Main_dataset and droping all the null values , and do extensive data exploration on them.
   
3. Feature engineering, Feature selection and model building to build the prediction model. Test the model and apply to previously split data containing null "exactPrice"

Here I am converting 9 to NaN values in the dataframe
"""



df = df.replace("9",np.nan)
df.head()

df.info()

df["carpetAreaUnit"].value_counts()

"""Since the Carpet area is having non consistent unit, We are going to change the carpet area to Sq-ft

"""

###Kanal carpet area in PAkistan multily by 5445
temp_dataset = df
selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Kanal"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Kanal"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Kanal

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Kanal"]), 'carpetArea'] = selected_rows['carpetArea'] * 5445

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Kanal"].head(5))

# -------------

selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Sq-yrd"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Sq-yrd"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Sq-yrd

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Sq-yrd"]), 'carpetArea'] = selected_rows['carpetArea'] * 9

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Sq-yrd"].head(5))

# ---------------

selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Sq-m"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Sq-m"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Sq-m

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Sq-m"]), 'carpetArea'] = selected_rows['carpetArea'] * 10.7639

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Sq-m"].head(5))

# ---------------

selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Marla"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Marla"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Marla used in Pakistan

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Marla"]), 'carpetArea'] = selected_rows['carpetArea'] * 272.251

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Marla"].head(5))

# ---------------

selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Biswa1"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Biswa1"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Biswa1 PAkistan,Bangladesh,India

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Biswa1"]), 'carpetArea'] = selected_rows['carpetArea'] * 1350

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Biswa1"].head(5))

# ---------------

selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Rood"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Rood"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Rood

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Rood"]), 'carpetArea'] = selected_rows['carpetArea'] * 272.25

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Rood"].head(5))

# ---------------

selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Biswa2"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Biswa2"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Biswa2

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Biswa2"]), 'carpetArea'] = selected_rows['carpetArea'] * 1350

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Biswa2"].head(5))

# ---------------

selected_rows = temp_dataset[temp_dataset['carpetAreaUnit'].isin(["Acre"])]
print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Acre"].head(5))

### MANIPULATING the values of carpetArea for the selected rows for Acre

temp_dataset.loc[temp_dataset['carpetAreaUnit'].isin(["Acre"]), 'carpetArea'] = selected_rows['carpetArea'] * 43560

print(temp_dataset["carpetArea"].loc[temp_dataset['carpetAreaUnit'] == "Acre"].head(5))

df.shape

"""Since Every carpet area is converted into sq-ft unit, we don't need "carpetAreaUnit" anymore so we are dropping it

"""

df = df.drop(["carpetAreaUnit"],axis=1)

df.shape



features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]

for feature in features_with_na:

    print(feature, np.round((df[feature].isnull().mean())*100, 4),  ' % missing values')

df.columns

"""Since "exactPrice" is the target column and it has very low null value percentage, I am spliting the Main_dataset into two datasets:

1.   dataset : Containing Main_dataset where there are no Null values in "exactPrice" column

2.   data_null : Containing Main_dataset where there are Null values in "exactPrice" column


"""

df["exactPrice"].isnull().sum()

data_null = df[df["exactPrice"].isnull() == True]

data_null.shape

data_null.head()

dataset = df.dropna(subset = ["exactPrice"])

dataset.head()

"""## In Data Analysis We will Analyze To Find out the below stuff

1. Missing Values

2. All The Numerical Variables

3. Distribution of the Numerical Variables

4. Outliers

5. Categorical Variables

6. Cardinality of Categorical Variables

7. Mapping data as per their Longitude and Latitude

####Missing Values
"""

features_with_na=[features for features in dataset.columns if dataset[features].isnull().sum()>1]

for feature in features_with_na:

    print(feature, np.round((dataset[feature].isnull().mean())*100, 4),  ' % missing values\n')

"""#### Here we can clearly see the that most of the data is null in amenity featues , more than 78% , maintenanceCharges 76% and no of Lifts 1.63 %

##### We are droping all the null values to see the effect of the features on exact price

```
# This is formatted as code
```


"""

dataset

dataset_dropped_na = dataset.dropna().copy()

dataset_dropped_na

"""#### A huge chunk of the data set is removed because of dropping null values, Lets explore this dataset 1st

### 2. All The Numerical Variables
"""

# list of numerical variables
numerical_features = [feature for feature in dataset_dropped_na.columns if dataset_dropped_na[feature].dtypes != 'O']

print('Number of numerical variables: ', len(numerical_features))

# visualise the numerical variables
dataset_dropped_na[numerical_features].head()

## Numerical variables are usually of 2 type
## 1. Continous variable and Discrete Variables

discrete_feature=[feature for feature in numerical_features if len(dataset_dropped_na[feature].unique())<25 and feature]
print("Discrete Variables Count: {}".format(len(discrete_feature)))

#  We are left with only houses for rent after dropping the null values
#  It proves that only 1530 out of 279000 properties have the information about all the features. i.e. 0.11% ,very rare

dataset_dropped_na["RentOrSale"].value_counts()

dataset_dropped_na[discrete_feature].head()

"""### 3. Distribution of the Numerical Variables"""

for feature in discrete_feature:
    data=dataset_dropped_na.copy()
    dataset_dropped_na.groupby(feature)['exactPrice'].median().plot.bar()
    plt.xlabel(feature)
    plt.ylabel('exactPrice')
    plt.title(feature)
    plt.show()

### Continuous Variable
continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]
print("Continuous feature Count {}".format(len(continuous_feature)))

#  Removing Latitude and Longitude since their main significance is to explore data based on geolocation
continuous_feature.remove("Lat")
continuous_feature.remove("Long")

## Lets analyse the continuous values by creating histograms to understand the distribution

for feature in continuous_feature:
    data=dataset_dropped_na.copy()
    dataset_dropped_na[feature].hist(bins=25)
    plt.xlabel(feature)
    plt.ylabel("Count")
    plt.title(feature)
    plt.show()

## We will be using logarithmic transformation


for feature in continuous_feature:
    data=dataset_dropped_na.copy()
    if 0 in data[feature].unique():
        pass
    else:
        data[feature]=np.log(data[feature])
        data['exactPrice']=np.log(data['exactPrice'])
        plt.scatter(data[feature],data['exactPrice'])
        plt.xlabel(feature)
        plt.ylabel('exactPrice')
        plt.title(feature)
        plt.show()

"""### 4. Outliers"""

for feature in continuous_feature:
    data=dataset.copy()
    if 0 in data[feature].unique():
        pass
    else:
        data[feature]=np.log(data[feature])
        data.boxplot(column=feature)
        plt.ylabel(feature)
        plt.title(feature)
        plt.show()

"""### 5. Categorical Variables"""

categorical_features=[feature for feature in dataset_dropped_na.columns if dataset_dropped_na[feature].dtypes=='O']
categorical_features

dataset_dropped_na[categorical_features].head()

for feature in categorical_features:
    print('The feature is {} and number of categories are {}'.format(feature,len(dataset_dropped_na[feature].unique())))

"""ou are removing certain features from the categorical_features list for specific reasons, likely based on your analysis of their characteristics or their relevance to your current exploration. Let me explain why you might want to remove each feature:

postedOn (54 categories):

This feature likely represents the date or timestamp when a property was posted. Dates are typically not useful as categorical features for modeling purposes, especially if you don't plan to analyze them as time-series data. Instead, you may want to convert them into more meaningful features like the day of the week, month, or year if relevant.
locality (114 categories):

Locality represents the area or neighborhood where the property is located, and it seems to have a large number of categories (114). High cardinality (many categories) can lead to sparsity in the data, and encoding such a feature as categorical might result in a high-dimensional feature that is not useful or might make it harder for models to generalize effectively. In some cases, this feature might be dropped or transformed (e.g., by grouping neighborhoods into broader regions).
URLs (362 categories):

This feature most likely represents URLs or web links to individual property listings. Since each listing's URL is unique, this feature will have a high cardinality, and using it as a categorical feature wouldn't make sense. It does not provide meaningful information for analysis or modeling, so it's better to remove it from the list.
RentOrSale (1 category):

This feature contains only a single category (either "Rent" or "Sale"). If there's only one unique value, it doesn't provide any useful variation or information, so it doesn't contribute to the model. Removing it makes sense to avoid introducing unnecessary noise.
Summary
You're removing these features because:

postedOn: likely needs to be treated as a date-time feature, not a categorical one.
locality: has high cardinality and might require special handling or grouping.
URLs: doesn't provide useful information for the model due to its uniqueness.
RentOrSale: has only one unique value, making it redundant.
These removals help streamline your analysis and modeling process by eliminating features that won't contribute meaningfully.
"""

### 6. Cardinality of Categorical Variables

for feature in categorical_features:
    data=dataset_dropped_na.copy()
    data.groupby(feature)['exactPrice'].median().plot.bar()
    plt.xlabel(feature)
    plt.ylabel('exactPrice')
    plt.title(feature)
    plt.show()

"""### 7. Mapping data as per their Longitude and Latitude"""

def preprocess_for_map(df) :
    df = df[['locality','Lat','Long','exactPrice',"city"]]
    # df = df.replace('NA', np.nan)
    df.dropna(subset=['Lat'], inplace=True)
    df.dropna(subset=['exactPrice'], inplace=True)
    df["Lat"] = df["Lat"].astype(float)
    df["Long"] = df["Long"].astype(float)
    return df


map_data = preprocess_for_map(dataset)

map_data[['Lat','Long','exactPrice',"city"]]

city_map = folium.Map(location=[21, 78], zoom_start=11.2,
                      tiles='Stamen Terrain',
                      attr='Map tiles by Stamen Design, under CC BY 3.0. Data by OpenStreetMap, under ODbL.')
mc = MarkerCluster()
for idx, row in map_data.iterrows():
    if not math.isnan(row['Long']) and not math.isnan(row['Lat']):
        popup = """
        Locality : <b>%s</b><br>
        Price : <b>%s</b><br>
        city : <b>%s</b><br>
        """ % (row['locality'], row['exactPrice'],row["city"])
        mc.add_child(Marker([row['Lat'], row['Long']],tooltip=popup))
    city_map.add_child(mc)
city_map

"""# Initiation of Step 3


### Uptil now we have explored data with no null value in all the 90 features. But this resulted into very low amount of data to explore
##### Now we are going to choose specific columns to drop the null values and for important features we would fill the null values to explore wider rande of data
##### Since the exactPrice of properties for Sale would be very high than that of rent, I am seperating the the dataset on rent or sale to get a better look in the characteristics of the features
"""

### Since Rent or Sale feature has very low null values 0.11%, I have decided to drop it from features_with_na and drop null values in dataset for smoother analysis
### Removing from features_with_na list

features_with_na.remove("RentOrSale")
features_with_na.remove("URLs")

### dropiing the features from the dataset

dataset = dataset.dropna(subset = ["RentOrSale"])
dataset = dataset.dropna(subset = ["URLs"])

"""#### Making an empty set to append all the important features/ those features where Null values scored more than the not Null values, Since those features are important and should not be droped the null values right away

#### Initializing a set of important features
"""

impFea =set()

type(impFea)

"""### Here we check If the null values in any any feature correlates to higher exactPrice and store them in a set for further analysis"""

for feature in features_with_na:
    data = dataset.copy()

    data[feature] = np.where(data[feature].isnull(), 1, 0)
    print(feature, (np.round(df[feature].isnull().mean(), 4)*100),  ' % missing values')

    bar_width = 0.4
    # grouped_data = data.groupby(feature)['exactPrice'].median()

    # Create a multibar plot
    category_data_rent = data[data['RentOrSale'] == "Rent"]

    category_data_sale = data[data['RentOrSale'] == "Sale"]

    dict = {0 :"Not Null", 1 :"Null"}

    def keys(x):
        key = dict[x]
        return key

    ### Rent subplot
    plt.figure(figsize = (10,5))
    plt.subplot(1,2,1)
    median_values = category_data_rent.groupby(feature)['exactPrice'].median()

    # since here 0 represents Not Null values and 1 represents Null values
    LIST = list(median_values.index)
    x = list(map(keys,LIST))

    if len(x) == 1:
        bar_width = 4
        if x == ["Null"]:
            impFea.add(feature)
    else:
         if median_values[1] > median_values[0]:
              impFea.add(feature)

    plt.bar(x, median_values, width=bar_width, label="Rent")

    for i,val in enumerate(LIST):
        plt.text(val,median_values[val],median_values[val],ha = "center", va = "bottom")

    str = f"{feature} focusing on Rent"
    plt.xlabel(str)
    plt.ylabel('Median Exact Price')


    ### Sale subplot
    plt.subplot(1,2,2)
    bar_width = 0.4
    median_values = category_data_sale.groupby(feature)['exactPrice'].median()

    # since here 0 represents Not Null values and 1 represents Null values
    LIST = list(median_values.index)
    x = list(map(keys,LIST))

    if len(x) == 1:
        bar_width = 4
        if x == ["Null"]:
            impFea.add(feature)


    else:
         if median_values[1] > median_values[0]:
              impFea.add(feature)

    plt.bar(x, median_values, width=bar_width, label="Sale")


    for i,val in enumerate(LIST):
        plt.text(val,median_values[val],median_values[val],ha = "center", va = "bottom")

    str = f"{feature} focusing on Sale"
    plt.xlabel(str)
    plt.ylabel('Median Exact Price')



    # Show the plot
    plt.show()

#  We got the important features
impFea

#### Using the postedOn feature to make new feature postedOn_DaysAgo, to represent how long ago the property was poster on the website
import datetime

# date_string = "Jun 20, '23"
def date_transform(date):

    date_object = datetime.datetime.strptime(date, "%b %d, '%y")
    formatted_date = datetime.datetime.strftime(date_object,"%Y-%m-%d")
    new_date = datetime.datetime.strptime(formatted_date, "%Y-%m-%d").date()


    current_date = datetime.date.today()
    ab = (current_date - new_date).days
    return ab


date_transform(dataset["postedOn"][10])

dataset["postedOn_DaysAgo"] = dataset["postedOn"].apply(date_transform)

dataset.info()

"""### Filling the null values with  missing for important features"""

dataset[list(impFea)] = dataset[list(impFea)].fillna(value = "Missing")

dataset.info()

features_with_na=[features for features in dataset.columns if dataset[features].isnull().sum()>1]

for feature in features_with_na:

    print(feature, np.round((dataset[feature].isnull().mean())*100, 4),  ' % missing values\n')

"""### Filling the null values with  missing for the features with low null values, less than 10%"""

fea_low_null =[]

for feature in features_with_na:

    if np.round((dataset[feature].isnull().mean())*100, 4)<10:
        print(feature)
        fea_low_null.append(feature)

dataset[fea_low_null] = dataset[fea_low_null].fillna(value= "Missing")

Not_null_fea = []

for feature in dataset.columns:
    if np.around(dataset[feature].isnull().sum()) == 0:
        Not_null_fea.append(feature)

Not_null_fea.remove("URLs")
Not_null_fea.remove("postedOn")

dataset_final = dataset[Not_null_fea]

dataset_final.isna().sum()

dataset_final

"""### Changing categorical_features to numeric features for model building"""

categorical_features=[feature for feature in dataset_final.columns if dataset_final[feature].dtype=='O']

for feature in categorical_features:
    data =dataset_final
    labels_ordered=data.groupby([feature])['exactPrice'].mean().sort_values().index
    labels_ordered={k:i for i,k in enumerate(labels_ordered,0)}
    data[feature]=data[feature].map(labels_ordered)

data

scaling_feature=[feature for feature in dataset_final.columns if feature not in ['exactPrice'] ]
len(scaling_feature)

"""### Since I am going to make a web app for people to predict cost of their preferencial houses, I am discarding the features['sqftPrice','Long','Lat', 'securityDeposit','firstMonthCharges', 'maintenanceCharges']"""

scaling_feature = [feature for feature in scaling_feature if feature not in ['sqftPrice','Long','Lat', 'securityDeposit','firstMonthCharges',"maintenanceCharges" ]]

scaling_feature

### Since I am going to make a web app for people to predict cost of their preferencial houses, I am discarding the features['sqftPrice','Long','Lat', 'securityDeposit','firstMonthCharges', 'maintenanceCharges']

scaling_feature

"""### Scaling features"""

from sklearn.preprocessing import MinMaxScaler
scaler=MinMaxScaler()
scaler.fit(dataset_final[scaling_feature])

scaler.transform(dataset_final[scaling_feature])

data = pd.concat([data[["exactPrice"]].reset_index(drop=True),
                    pd.DataFrame(scaler.transform(data[scaling_feature]), columns=scaling_feature)],
                    axis=1)

data

from sklearn.model_selection import train_test_split

x= data[scaling_feature]
y = np.log(data["exactPrice"])

max(y)

min(y)

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)

print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)

print(type(x_train))
print(type(y_train))
print(type(x_test))
print(type(y_test))

x_train.head()

y_train.head()

x_train.columns

y_test

len(y_test)

"""### Choosing best Regressor algorithm for my dataset"""

import sklearn
import xgboost

print("scikit-learn version:", sklearn.__version__)
print("xgboost version:", xgboost.__version__)

!pip install --upgrade scikit-learn
!pip install --upgrade xgboost

import numpy as np
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

# Define a function to train and evaluate models
def train_model(m, name, x_train, y_train, x_test, y_test):
    # Train the model
    m.fit(x_train, y_train)
    # Make predictions on the test set
    predictions = m.predict(x_test)
    # Calculate evaluation metrics
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    # Print the performance of the model
    print(f"{name} mae: {mae} r2: {r2}")
    return r2, mae, predictions  # Return metrics and predictions

# Assuming you have the training and testing data
# Example data split:
# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create instances of different models

reg1 = XGBRegressor(n_estimators=1000)
reg2 = RandomForestRegressor(n_estimators=1000)
reg3 = DecisionTreeRegressor()
reg4 = GradientBoostingRegressor(random_state=1)
reg5 = ExtraTreesRegressor()
reg6 = LinearRegression()


# Train the models and get their performance metrics
r2_1, mae_1, pred1 = train_model(reg1, "XGBRegressor", x_train, y_train, x_test, y_test)
r2_2, mae_2, pred2 = train_model(reg2, "RandomForestRegressor", x_train, y_train, x_test, y_test)
r2_3, mae_3, pred3 = train_model(reg3, "DecisionTreeRegressor", x_train, y_train, x_test, y_test)
r2_4, mae_4, pred4 = train_model(reg4, "GradientBoostingRegressor", x_train, y_train, x_test, y_test)
r2_5, mae_5, pred5 = train_model(reg5, "ExtraTreesRegressor", x_train, y_train, x_test, y_test)
r2_6, mae_6, pred6 = train_model(reg6, "LinearRegression", x_train, y_train, x_test, y_test)

# Manually combine the predictions by averaging
# Use the best models' predictions (based on R2 score or other metrics) to create an ensemble prediction

models = [pred1, pred2, pred3, pred4, pred5, pred6]
r2_scores = [r2_1, r2_2, r2_3, r2_4, r2_5, r2_6]
mae_scores = [mae_1, mae_2, mae_3, mae_4, mae_5, mae_6]

# Select the best models based on R2 score or mae (you can choose any criterion)
best_models = np.argsort(r2_scores)[::-1][:3]  # Select top 3 models based on R2 score

# Manually combine predictions from the top 3 models
ensemble_predictions = np.mean([models[i] for i in best_models], axis=0)

# Evaluate the ensemble model
ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)
ensemble_r2 = r2_score(y_test, ensemble_predictions)
print(f"Ensemble model mae: {ensemble_mae} r2: {ensemble_r2}")

import numpy as np
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from xgboost import XGBRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split

# Define a function to train and evaluate models
def train_model(m, name, x_train, y_train, x_test, y_test):
    # Train the model
    m.fit(x_train, y_train)
    # Make predictions on the test set
    predictions = m.predict(x_test)
    # Calculate evaluation metrics
    mae = mean_absolute_error(y_test, predictions)
    r2 = r2_score(y_test, predictions)
    # Print the performance of the model
    print(f"{name} mae: {mae} r2: {r2}")
    return r2, mae, predictions  # Return metrics and predictions

# Assuming you have the training and testing data
# Example data split:
# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create instances of different models
models = {
    "XGBRegressor": XGBRegressor(n_estimators=1000),
    "RandomForestRegressor": RandomForestRegressor(n_estimators=1000),
    "DecisionTreeRegressor": DecisionTreeRegressor(),
    "GradientBoostingRegressor": GradientBoostingRegressor(random_state=1),
    "ExtraTreesRegressor": ExtraTreesRegressor(),
    "LinearRegression": LinearRegression()
}

# Train and evaluate each model
results = {}
for name, model in models.items():
    r2, mae, predictions = train_model(model, name, x_train, y_train, x_test, y_test)
    results[name] = {'R2': r2, 'MAE': mae, 'predictions': predictions}

# Find the best model based on R2 score (or MAE)
best_model_r2 = max(results, key=lambda x: results[x]['R2'])  # Best model by R2 score
best_model_mae = min(results, key=lambda x: results[x]['MAE'])  # Best model by MAE score

print(f"The best model based on R2 score is: {best_model_r2}")
print(f"The best model based on MAE is: {best_model_mae}")

# If you want to combine the predictions from top models, select top N models
# Select the top 3 models based on R2 score
top_n_models = np.argsort([results[name]['R2'] for name in models])[-3:]  # Top 3 models based on R2 score
top_models = [list(models.keys())[i] for i in top_n_models]

# Manually combine predictions from the top models
ensemble_predictions = np.mean([results[model]['predictions'] for model in top_models], axis=0)

# Evaluate the ensemble model
ensemble_mae = mean_absolute_error(y_test, ensemble_predictions)
ensemble_r2 = r2_score(y_test, ensemble_predictions)
print(f"Ensemble model mae: {ensemble_mae} r2: {ensemble_r2}")

"""#### Looks like the XGBoost Regressor has the best scores, lowest mean_absolute_error (0.288...) and highest r2 score (0.982...).
##### So, I am chossing XGBoost Regressor as the main algorithm
##### Keeping in mind that I need to convert that exactPrice feature using np.exp to give valid output,  its essential to have low mean_absolute_error
"""

rfr = RandomForestRegressor(n_estimators=1000).fit(x_train, y_train)


predictions = rfr.predict(x_test)
r2 = r2_score(y_test, predictions)

print(r2*100,"%")

from sklearn.metrics import mean_absolute_error,r2_score,classification_report

fi = pd.DataFrame(rfr.feature_importances_,
             columns=['importance'])
fi['feature'] = scaling_feature
fi = fi.sort_values('importance', ascending=False)

top_10_features = fi.head(10)

plt.figure(figsize=(20, 10))
ax = sns.barplot(data=top_10_features, x='importance', y='feature',
                 palette="spring_r")
ax.tick_params(axis='both', which='both', labelsize=15)
ax.set_xlabel('Importance',fontsize=15, weight="bold")
ax.set_ylabel('Feature',fontsize=15,weight="bold")
plt.title("Feature Importance", size=20, weight="bold")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

# Assuming dataset is already loaded in 'dataset'
dataset = pd.read_csv('Scraped_Data.csv')
# 1. Identify categorical columns (you can use `df.select_dtypes` to check which columns are of type 'object')
categorical_columns = dataset.select_dtypes(include=['object']).columns

# 2. Apply One-Hot Encoding to categorical features
dataset_encoded = pd.get_dummies(dataset, columns=categorical_columns, drop_first=True)

# 3. Split the data into train and test (keeping 'exactPrice' as the target)
train_data = dataset_encoded[dataset_encoded['exactPrice'].notnull()]
test_data = dataset_encoded[dataset_encoded['exactPrice'].isnull()]

X_train = train_data.drop(columns=['exactPrice'])
y_train = train_data['exactPrice']
X_test = test_data.drop(columns=['exactPrice'])

# 4. Train the model (RandomForestRegressor or any other model you've chosen)
model = RandomForestRegressor(n_estimators=1000, random_state=42)
model.fit(X_train, y_train)  # Fit the model on the data with known 'exactPrice'

# 5. Make predictions for the test data (houses with missing 'exactPrice')
predicted_prices = model.predict(X_test)

# 6. Insert the predicted prices back into the test_data dataframe
test_data['predictedPrice'] = predicted_prices

# 7. Combine the predicted data with the original data (optional)
final_predictions = pd.concat([train_data, test_data])

# 8. Print or examine the predictions
print(final_predictions[['predictedPrice']].head())